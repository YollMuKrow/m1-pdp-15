
Plan :


I/Introduction
2/Description et Analyse de l'existant (besoins fonctionnels de l'application existante)
3/Environnement de développement
4/Analyse des besoins
  1.1 Slime
  1.2 Robocup
  1.3 Liaison (optionnel)
5/Logiciel et Architecture
  2.1 Slime
  2.2 Robocup
6/Analyse du fonctionnement et Tests
  3.1 Slime
  3.2 Robocup
7/Défauts et bugs
  4.1 Slime
  4.2 Robocup
6/Conclusion
7/Bibliographie





2/ Description et Analyse de l'existant

Une partie cruciale du projet consiste dans un premier temps à analyser les fonctionnalités existantes de SlimeVolley, un projet github.

Le jeu est très simple : le but de l'agent est de faire atterrir le ballon sur le sol de son adversaire, faisant perdre une vie à son adversaire.
Chaque agent commence avec cinq vies. L'épisode se termine lorsque l'un ou l'autre des agents perd ses cinq vies ou après que 3000 pas de temps se sont écoulés.
Un agent reçoit une récompense de +1 lorsque son adversaire perd ou de -1 lorsqu'il perd une vie.

Cet environnement est basé sur "Neural Slime Volleyball" (https://otoro.net/slimevolley/), un jeu JavaScript créer en 2015 (https://blog.otoro.net/2015/03/28/neural -slime-volleyball /)
qui utilise le self-play (un agent apprend à jouer contre soi-même) et l'évolution (evolution strategies ex : cma-es) pour former un simple agent de réseau neuronal à mieux jouer au jeu que la plupart des joueurs humains.
L'environnement a été porté sur python, car c'est un environnement léger et rapide et gym en tant que banc d'essai pour des méthodes de reinforcement learning plus avancées telles que les algorithmes multi-agents, self-play,
d'apprentissage continu (continual learning) et d'apprentissage par imitation (imitation learning algorithms).

En effet comme les deux jeux sont assez semblables d'après les règles de base (marquer dans le camp adverse), la représentation maquette donnée (slimes, interactions avec la balle),
les environnements assez similaires (zones de marquage par exemple), il est nécessaire de déterminer quelles sont
les fonctionnalités déjà existantes, quelles sont celles que l'on va pouvoir réutiliser, celles que l'on
va devoir adapter ou peaufiner.
Cela doit nous permettre plus rapidement de construire un environnement paramétrable adéquat représentant une version simplifiée de la RobotCup au lieu de construire un jeu 2D du début,
puis de finalement rentrer dans la partie la plus intéressante du projet qui consistera à étudier et à utiliser divers d'algorithmes d'apprentissage, établir des
benchmarks, étudier l'impact de la modification des règles dans le comportement des agents.

Déterminons donc au préalable les besoins fonctionnels existants afin d'établir une liaison avec certains besoins fonctionnels de notre application.

Besoins fonctionnels (existants):

    Mettre en place plusieurs types d'environnement SlimeVolley:

        Il faut implémenter l'interface Env du module gym pour pouvoir créer un environnement personnalisé, respecter les consignes pour qu'il soit correctement interprétable par les
        différents algorithmes disponibles dans la bibliothèque stable_baselines.
        Il faut définir plusieurs attributs de base qui sont réutilisés dans les algorithmes d'apprentissage par renforcement pour déterminer l'action la plus adéquate, par exemple
        pour l'algorithme PPO qui cherche à optimiser la politique (l'action a menée).
        Tel que préciser dans la documentation de gym, il est préférable d'éviter les versions customisés d'espaces et réutiliser les espaces existants spécifiés dans l'interface Space du module Gym.

        Définir l'espace d'actions :
            Typiquement, il faut définir les trois actions possibles pour un agent donné : avancer, reculer, sauter.

        Construire différents types d'espaces d'observations :

            Mode d'observation par états : On représente ici les deux agents et la balle par un vecteur à 12 dimensions
            [x_agent, y_agent, vx_agent, vy_agent, x_ball, y_ball, vx_ball, vy_ball, x_opponent, y_opponent, vx_opponent, vy_opponent].

            Mode d'observation par pixels :






    Définir la structure de notre jeu :






























3/ Environnement de développement

Langage de programmation : Le langage de programmation est Python, le langage est facile d'utilisation,
il a un haut niveau d'abstraction et propose de nombreuses bibliothèques
permettant de faciliter la mise en place de différents types de projets dans le machine learning (deep learning, reinforcement learning),
c'est pour cela qu'il est très utilisé par les chercheurs qui ne sont pas forcément informaticiens, de part sa simplicité, il permet d'implémenter des algorithmes et d'obtenir des
résultats rapidement sans se soucier des détails techniques de bas niveau (gestion de la mémoire, du multithreading entre autres).

Bibliothèques :

    Gym : C'est un boîte à outils d'OpenAI, cette société a pour vocation de faciliter la création d'environnements pour développer et comparer des algorithmes d'apprentissage par renforcement.
    En effet, à la différence de l'apprentissage supervisé où les progrès ont été conséquent grâce à la manipulation de grands ensembles de données (images), en apprentissage par renforcement on ne dispose
    actuellement pas d'une collection suffisante d'environnements variés, facile à paramétrer et à utiliser.
    Il est nécessaire également de standardiser les environnements car le moindre changement dans l'espace d'actions, ou dans le système de récompenses peut avoir de grandes conséquences. Cela rend difficile
    de reproduire les travaux de différentes publications et de comparer les résultats.
    Cette bibliothèque permet donc un haut niveau d'abstraction, il suffit d'implémenter l'interface, définir notre espace d'actions,
    notre espace d'observations, définir l'intervalle de récompenses, implémenter les méthodes qui permettent de contrôler notre environnement, son cycle de vie etc...

    NumPy : C'est une bibliothèque permettant de travailler avec tableaux, matrices.
    En Python, nous avons des listes qui servent à représenter des tableaux, mais elles sont lentes à traiter.
    Les tableaux NumPy sont stockés à un endroit continu de la mémoire contrairement aux listes, de sorte que les processus peuvent y accéder et les manipuler très efficacement.
    Ce comportement est appelé 'locality of reference' en informatique.
    NumPy vise à fournir un objet de tableau jusqu'à 50 fois plus rapides que les listes Python traditionnelles,
    il fournit de nombreuses fonctions de support qui facilitent le travail avec les tableaux.
    Les tableaux sont très fréquemment utilisés dans la science des données, où la vitesse et les ressources sont très importants.

    stable-baselines : C'est un ensemble d'implémentations améliorées d'algorithmes d'apprentissage par renforcement basés sur OpenAI. L'algorithme PPO est
    par exemple utilisé dans le code existant de SlimeVolley.
    Ces algorithmes peuvent permettre à différents chercheurs de répliquer, d'affiner et d'identifier de nouvelles idées plus facilement et
    et donc créer de bonnes bases de référence pour construire des projets.
    Bien qu'au début, du fait du manque de connaissances dans le domaine de l'apprentissage par renforcement, cette bibliothèque va agir comme une
    "boîte noire". À partir de notre environnement, des observations actuelles, elle pourra prédire l'action la plus adéquate pour un agent donné sans vraiment
    comprendre correctement tous les concepts derrière.
